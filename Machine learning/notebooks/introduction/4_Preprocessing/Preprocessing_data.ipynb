{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "The objective of this notebook is to introduce how to preprocess data. Data Preprocessing is a technique that is used to transform our data from one format to another that is more suitable for analysis. The data transformations are typically applied to distinct entities (e.g. fields, rows, columns, data values etc.) within a data set, and could include such actions as parsing, filtering, normalization, standarization, etc.\n",
    "\n",
    "* **Example.** In the \"Automobile\" dataset, what is the fuel consumption (L/100k) rate for the diesel car?\n",
    "* **Example.** What should we do with missing values?\n",
    "\n",
    "#### Required libraries\n",
    "* <a href = \"https://pandas.pydata.org/\"><code>Pandas</code></a>\n",
    "* <a href = \"https://www.scipy.org/\"><code>Scipy</code></a> \n",
    "* <a href = \"https://numpy.org/\"><code>Numpy</code></a> \n",
    "* <a href = \"https://numpy.org/\"><code>Scikit-learn</code></a>\n",
    "* <a href = \"https://matplotlib.org/\"><code>Matplotlib</code></a>\n",
    "\n",
    "#### Table of contents\n",
    "<ol>\n",
    "    <li><a href=\"#identify_handle_missing_values\">Identify and handle missing values</a><br>\n",
    "        1.1. <a href=\"#identify_missing_values\">Identify missing values</a><br>\n",
    "        1.2. <a href=\"#deal_missing_values\">Deal with missing values</a><br>\n",
    "    </li>\n",
    "    <li><a href=\"#data_formatting\">Data formatting</a></li>\n",
    "    <li><a href=\"#data_transformation\">Data transformation</a><br>\n",
    "        3.1. <a href=\"#feature_scaling\">Feature scaling</a><br>\n",
    "        3.2. <a href=\"#transforming_data_to_normality\">Transforming data to normality</a>\n",
    "    </li>\n",
    "    <li><a href=\"#data_binning\">Data binning</a></li>\n",
    "    <li><a href=\"#indicator_variables\">Indicator variables</a></li>\n",
    "</ol>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"identify_handle_missing_values\">1 - Identify and handle missing values</h2>\n",
    "\n",
    "\n",
    "<h3 id=\"identify_missing_values\">1.1 - Identify missing values</h3>\n",
    "In the <code>Automobile</code> dataset, missing data comes with the question mark \"?\". We replace \"?\" with NaN (Not a Number), which is Python's default missing value marker, for reasons of computational speed and convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel-type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num-of-doors</th>\n",
       "      <th>body-style</th>\n",
       "      <th>drive-wheels</th>\n",
       "      <th>engine-location</th>\n",
       "      <th>wheel-base</th>\n",
       "      <th>...</th>\n",
       "      <th>engine-size</th>\n",
       "      <th>fuel-system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111</td>\n",
       "      <td>5000</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>13495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111</td>\n",
       "      <td>5000</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>16500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>94.5</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154</td>\n",
       "      <td>5000</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>16500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>audi</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   symboling normalized-losses         make fuel-type aspiration num-of-doors  \\\n",
       "0          3               NaN  alfa-romero       gas        std          two   \n",
       "1          3               NaN  alfa-romero       gas        std          two   \n",
       "2          1               NaN  alfa-romero       gas        std          two   \n",
       "3          2               164         audi       gas        std         four   \n",
       "4          2               164         audi       gas        std         four   \n",
       "\n",
       "    body-style drive-wheels engine-location  wheel-base  ...  engine-size  \\\n",
       "0  convertible          rwd           front        88.6  ...          130   \n",
       "1  convertible          rwd           front        88.6  ...          130   \n",
       "2    hatchback          rwd           front        94.5  ...          152   \n",
       "3        sedan          fwd           front        99.8  ...          109   \n",
       "4        sedan          4wd           front        99.4  ...          136   \n",
       "\n",
       "   fuel-system  bore  stroke compression-ratio horsepower  peak-rpm city-mpg  \\\n",
       "0         mpfi  3.47    2.68               9.0        111      5000       21   \n",
       "1         mpfi  3.47    2.68               9.0        111      5000       21   \n",
       "2         mpfi  2.68    3.47               9.0        154      5000       19   \n",
       "3         mpfi  3.19     3.4              10.0        102      5500       24   \n",
       "4         mpfi  3.19     3.4               8.0        115      5500       18   \n",
       "\n",
       "  highway-mpg  price  \n",
       "0          27  13495  \n",
       "1          27  16500  \n",
       "2          26  16500  \n",
       "3          30  13950  \n",
       "4          22  17450  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../Data/Automobile.csv\")\n",
    "\n",
    "# replace \"?\" to NaN\n",
    "df.replace(\"?\", np.nan, inplace = True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values are converted to Python's default. We use Python's built-in functions to identify these missing values. There are two methods to detect missing data:\n",
    "\n",
    "* <code>.isnull()</code>\n",
    "* <code>.notnull()</code>\n",
    "\n",
    "The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull()\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count missing values in each column\n",
    "Using a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, <code>True</code> represents a missing value, <code>False</code> means the value is present in the dataset.  In the body of the for loop the method  <code>.value_counts()</code> counts the number of <code>True</code> values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data[column].value_counts())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative is to use a list comprehension to create a list of tuples (column_name, count):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_counts = [(column, missing_data[column].values.sum()) for column in missing_data.columns]\n",
    "false_counts = [(column, (~missing_data[column].values).sum()) for column in missing_data.columns]\n",
    "true_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary above, each column has 205 rows of data, seven columns containing missing data:\n",
    "\n",
    "* \"normalized-losses\": 41 missing data\n",
    "* \"num-of-doors\": 2 missing data\n",
    "* \"bore\": 4 missing data\n",
    "* \"stroke\" : 4 missing data\n",
    "* \"horsepower\": 2 missing data\n",
    "* \"peak-rpm\": 2 missing data\n",
    "* \"price\": 4 missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"deal_missing_values\">1.2 - Deal with missing data</h3>\n",
    "When dealing with missing data we have several options we have two options, drop or imputation, depending on the situation and our objective we should apply one or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop data\n",
    "Whole columns should be dropped only if most entries in the column are empty (same goes for rows). In our dataset, none of the columns are empty enough to drop entirely. There are, however, 4 rows with missing data in the \"price\" column. One way to solve this would be to simply drop those rows.\n",
    "\n",
    "In order to drop a specific column, we simply use the <code>drop</code> method in <code>DataFrame</code> (independently of its missing values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the normalized-losses column\n",
    "\n",
    "print(df.shape)\n",
    "column_dropped_df = df.drop(columns=[\"normalized-losses\"])\n",
    "print(column_dropped_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to drop only the columns with missing values, we can simply use the method <code>dropna</code> in <code>DataFrame</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "column_dropped_df = df.dropna(axis='columns')\n",
    "print(column_dropped_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can drop those rows that have a missing value in the an specific attribute (i.e., \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "row_dropped_df = df.dropna(subset=[\"price\"])\n",
    "print(row_dropped_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute data\n",
    "Discarding entire rows and/or columns containing missing values comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute those values, i.e., to infer them from the known part of the data.\n",
    "\n",
    "One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension. By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values.\n",
    "\n",
    "**Univariate imputation**\n",
    "\n",
    "The <code>SimpleImputer</code> class provides basic univariate strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"normalized-losses mean: \" +str(df['normalized-losses'].astype('float').mean(axis=0)))\n",
    "\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputed_normalized_losses = mean_imputer.fit_transform(df[[\"normalized-losses\"]]) # It expects DataFrame (not a Series object)\n",
    "pd.DataFrame(imputed_normalized_losses).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"most frequent num-doors value: \" + str(df['num-of-doors'].value_counts().idxmax()))\n",
    "\n",
    "freq_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputed_num_doors = freq_imputer.fit_transform(df[[\"num-of-doors\"]]) # It expects DataFrame (not a Series object)\n",
    "pd.DataFrame(imputed_num_doors).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multivariate imputation**\n",
    "\n",
    "A more sophisticated approach is to use multivariate imputation where we use information from other data attributes to impute the value.\n",
    "* <code>IterativeImputer</code> models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A **regressor** (*linear*, *Random Forest*, etc.) is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\n",
    "* <code>KNNImputer</code> provides imputation for filling in missing values using the k-Nearest Neighbors approach. Each missing feature is imputed using values from nearest neighbors that have a value for the feature. If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other imputation approaches**\n",
    "\n",
    "There multiple methods of imputation, another common approach is the <a href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\">**expectation-maximization**</a> (EM) algorithm, which can be used with discrete and continuous data. We will not cover it in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Question: Which one should we choose?**\n",
    "\n",
    "(double click for solution)\n",
    "\n",
    "<!--\n",
    "There isn't a \"best\" method. If possible, the best alternative is to test all of them on our data. Some methods could work better on a DataSet and worse in another. For example:\n",
    "\n",
    "<img src=\"images/example_imputer.png\" width=\"800\">\n",
    "-->\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to our example**\n",
    "\n",
    "Given the introductory nature of this tutorial, we are going to opt for simplicity. Therefore we are going to use univariate imputation, where continuous missing values are going to be replaced by the mean value of its column, and discrete missing values are going to be replaced by the most frequent value of the column. In addition, we are goin to drop those rows with a missing value in the \"price\" attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "freq_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "df[\"normalized-losses\"] = mean_imputer.fit_transform(df[[\"normalized-losses\"]])\n",
    "df[\"bore\"] = mean_imputer.fit_transform(df[[\"bore\"]])\n",
    "df[\"stroke\"] = mean_imputer.fit_transform(df[[\"stroke\"]])\n",
    "df[\"horsepower\"] = mean_imputer.fit_transform(df[[\"horsepower\"]])\n",
    "df[\"peak-rpm\"] = mean_imputer.fit_transform(df[[\"peak-rpm\"]])\n",
    "df[\"num-of-doors\"] = freq_imputer.fit_transform(df[[\"num-of-doors\"]])\n",
    "df = df.dropna(subset=[\"price\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we continue...\n",
    "Lets just save our imputed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Automobile_nomissing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_formatting\">2 - Data formatting</h2>\n",
    "An important step when cleaning our data is to check and make sure that all data is in the correct format (int, float, text or other). In Pandas, we use \n",
    "\n",
    "* <code>.dtypes</code> attribute to check the data types\n",
    "* <code>.astype()</code> to change a data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, some columns are not of the correct data type. Numerical variables should have type <code>float</code> or <code>int</code>, and variables with strings such as categories should have type <code>object</code>. For example, \"price\" should be <code>float</code> and \"normalized-losses\" should be <code>int</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"normalized-losses\"]] = df[[\"normalized-losses\"]].astype(\"int\")\n",
    "df[[\"price\"]] = df[[\"price\"]].astype(\"float\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** you will notice that most of the types are in 64 bits. In this example we could reduce it to 32 bits given that we dont require that much precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature transformation\n",
    "\n",
    "Data is usually collected from different agencies with different formats. It may be interesting to transform this data into a common format that allows the researcher to make meaningful comparisons.\n",
    "\n",
    "**Example:** Transform mpg to L/100km\n",
    "\n",
    "In our dataset, the fuel consumption columns \"city-mpg\" and \"highway-mpg\" are represented by mpg (miles per gallon) unit. Assume we are developing an application in a country that accept the fuel consumption with L/100km standard.\n",
    "\n",
    "The formula for unit conversion is\n",
    "\n",
    "$$ L/100km = \\frac{235}{mpg}$$\n",
    "\n",
    "We can do many mathematical operations directly in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"city-mpg\", \"highway-mpg\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"city-L/100km\"] = 235/df[\"city-mpg\"]\n",
    "df[\"highway-L/100km\"] = 235/df[\"highway-mpg\"]\n",
    "df[[\"city-L/100km\", \"highway-L/100km\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape) # 2 new columns have been added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_normalization\">3 - Data transformation</h2>\n",
    "\n",
    "<h3 id=\"feature_scaling\">3.1 - Feature scaling</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns of various distributions\n",
    "fs_df = pd.DataFrame({ \n",
    "    'beta': np.random.beta(5, 1, 1000) * 60,        # beta\n",
    "    'exponential': np.random.exponential(10, 1000), # exponential\n",
    "    'normal_p': np.random.normal(10, 2, 1000),      # normal platykurtic\n",
    "    'normal_l': np.random.normal(-10, 10, 1000),     # normal leptokurtic\n",
    "})\n",
    "\n",
    "# make bimodal distribution\n",
    "first_half = np.random.normal(20, 3, 500) \n",
    "second_half = np.random.normal(-20, 3, 500) \n",
    "bimodal = np.concatenate([first_half, second_half])\n",
    "\n",
    "fs_df['bimodal'] = bimodal\n",
    "\n",
    "# create list of column names to use later\n",
    "col_names = list(fs_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plot original distribution plot\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(6, 4))\n",
    "ax1.set_title('Original Distributions')\n",
    "\n",
    "sns.kdeplot(fs_df['beta'], ax=ax1)\n",
    "sns.kdeplot(fs_df['exponential'], ax=ax1)\n",
    "sns.kdeplot(fs_df['normal_p'], ax=ax1)\n",
    "sns.kdeplot(fs_df['normal_l'], ax=ax1)\n",
    "sns.kdeplot(fs_df['bimodal'], ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaler\n",
    "Also known as feature standarization, it makes the values of each feature in the data have **zero-mean** and **unit-variance**. The reasoning behind it is that if a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function of an estimator and make it unable to learn from other features correctly. For example, it may be applied before using a L1 normalizer in linear regresison or before applying principal component analysis (PCA).\n",
    "\n",
    "While it assumes that data follows a Gaussian distribution, in practice we often ignore the shape of the distribution and just transform the data to center it:\n",
    "\n",
    "$$\\mathbf{x}' = \\frac{\\mathbf{x} - \\mathbf{\\bar{x}}}{\\sigma(\\mathbf{x})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "s_scaler = StandardScaler()\n",
    "df_s = s_scaler.fit_transform(fs_df)\n",
    "\n",
    "df_s = pd.DataFrame(df_s, columns=col_names)\n",
    "\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(6, 4))\n",
    "ax1.set_title('After StandardScaler')\n",
    "\n",
    "sns.kdeplot(df_s['beta'], ax=ax1)\n",
    "sns.kdeplot(df_s['exponential'], ax=ax1)\n",
    "sns.kdeplot(df_s['normal_p'], ax=ax1)\n",
    "sns.kdeplot(df_s['normal_l'], ax=ax1)\n",
    "sns.kdeplot(df_s['bimodal'], ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-max scaler\n",
    "This method scales a feature in a user-specified range.  Selecting the target range depends on the nature of the data. Its general formula is given as:\n",
    "\n",
    "$$\\mathbf{x}' = \\frac{\\mathbf{x} - min(\\mathbf{x})}{max(\\mathbf{x}) - min(\\mathbf{x})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mm_scaler = MinMaxScaler([-1,1])\n",
    "df_mm = mm_scaler.fit_transform(fs_df)\n",
    "\n",
    "df_mm = pd.DataFrame(df_mm, columns=col_names)\n",
    "\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(6, 4))\n",
    "ax1.set_title('After MinMaxScaler')\n",
    "\n",
    "sns.kdeplot(df_mm['beta'], ax=ax1)\n",
    "sns.kdeplot(df_mm['exponential'], ax=ax1)\n",
    "sns.kdeplot(df_mm['normal_p'], ax=ax1)\n",
    "sns.kdeplot(df_mm['normal_l'], ax=ax1)\n",
    "sns.kdeplot(df_mm['bimodal'], ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robust scaler\n",
    "This scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile $Q_{1}(\\mathbf{x})$ and the 3rd quartile $Q_{3}(\\mathbf{x})$:\n",
    "\n",
    "$$\\mathbf{x}' = \\frac{\\mathbf{x} - Q_{1}(\\mathbf{x})}{Q_{3}(\\mathbf{x}) - Q_{1}(\\mathbf{x})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "r_scaler = RobustScaler()\n",
    "df_r = r_scaler.fit_transform(fs_df)\n",
    "\n",
    "df_r = pd.DataFrame(df_r, columns=col_names)\n",
    "\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(6, 4))\n",
    "ax1.set_title('After RobustScaler')\n",
    "\n",
    "sns.kdeplot(df_r['beta'], ax=ax1)\n",
    "sns.kdeplot(df_r['exponential'], ax=ax1)\n",
    "sns.kdeplot(df_r['normal_p'], ax=ax1)\n",
    "sns.kdeplot(df_r['normal_l'], ax=ax1)\n",
    "sns.kdeplot(df_r['bimodal'], ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The influence of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "x = pd.DataFrame({\n",
    "    # Distribution with lower outliers\n",
    "    'x1': np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),\n",
    "    # Distribution with higher outliers\n",
    "    'x2': np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),\n",
    "})\n",
    "\n",
    "scaler = preprocessing.RobustScaler()\n",
    "robust_scaled_df = scaler.fit_transform(x)\n",
    "robust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['x1', 'x2'])\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "minmax_scaled_df = scaler.fit_transform(x)\n",
    "minmax_scaled_df = pd.DataFrame(minmax_scaled_df, columns=['x1', 'x2'])\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "standard_scaled_df = scaler.fit_transform(x)\n",
    "standard_scaled_df = pd.DataFrame(standard_scaled_df, columns=['x1', 'x2'])\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(14, 5))\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(x['x1'], ax=ax1)\n",
    "sns.kdeplot(x['x2'], ax=ax1)\n",
    "ax2.set_title('After Min-Max Scaling')\n",
    "sns.kdeplot(minmax_scaled_df['x1'], ax=ax2)\n",
    "sns.kdeplot(minmax_scaled_df['x2'], ax=ax2)\n",
    "ax3.set_title('After Robust Scaling')\n",
    "sns.kdeplot(robust_scaled_df['x1'], ax=ax3)\n",
    "sns.kdeplot(robust_scaled_df['x2'], ax=ax3)\n",
    "ax4.set_title('After Standard Scaling')\n",
    "sns.kdeplot(standard_scaled_df['x1'], ax=ax4)\n",
    "sns.kdeplot(standard_scaled_df['x2'], ax=ax4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"transforming_data_to_normality\">3.2 - Transforming data to normality</h3>\n",
    "\n",
    "One of the most common assumptions for statistical analyses is that of normality, with nearly all parametric analyses requiring this assumption in one way or another. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness. Some of the most common power transforms are:\n",
    "\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Power_transform#Yeo%E2%80%93Johnson_transformation\">The Yeo-Johnson transformation</a> (<code>sklearn.preprocessing.PowerTransformer(method=\"yeo-johnson\")</code>)\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation\">The Boxâ€“Cox transformation</a> (<code>sklearn.preprocessing.PowerTransformer(method=\"box-cox\")</code>)\n",
    "* <a href=\"https://stats.stackexchange.com/questions/325570/quantile-transformation-with-gaussian-distribution-sklearn-implementation/327102\">Quantile transformation</a> (<code>sklearn.preprocessing.QuantileTransformer</code>)\n",
    "\n",
    "Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation.\n",
    "\n",
    "<img src=\"images/power_transform.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion and a word of caution\n",
    "\n",
    "The usual advice when considering which transformer to use is the following:\n",
    "\n",
    "* Use min-max scaler as your default.\n",
    "* Use robust scaler if you have outliers and can handle a larger range.\n",
    "* Transform your data to normality and/or use standard scaler when your method specifically requires it.\n",
    "\n",
    "However, it is also interesting to learn our model using different transformations and compare their results.\n",
    "\n",
    "Finally, keep in mind that there is a \"hidden\" tradeoff in data transformation. It may now be centered and or normal, but interpreting that data may be much more difficult because you have to consider the previously applied transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_binning\">4 - Data binning</h2>\n",
    "\n",
    "Binning is a process of transforming continuous numerical variables into discrete categorical \"bins\". It can be done in a supervised or unsupervised manner. We are going to focus on the latter, which can be mainly divided in two approaches:\n",
    "* **Equal-width**. The algorithm divides the data into $k$ intervals of equal size.\n",
    "    * Pros: maintains the form of the distribution.\n",
    "    * Cons: very sensitive to outliers.\n",
    "* **Equal-frequency**. The algorithm divides the data into $k$ groups which each group contains approximately same number of values.\n",
    "    * Pros: resist outliers better.\n",
    "    * Cons: the marginal distribution becomes uniform.\n",
    "\n",
    "In our dataset, \"horsepower\" is a real valued variable ranging from 48 to 288, it has 57 unique values. What if we only care about the price difference between cars with high horsepower, medium horsepower, and little horsepower (3 types)? Can we rearrange them into three â€˜bins' to simplify analysis? \n",
    "\n",
    "We will use the scikit-learn to segment the 'horsepower' column into 3 bins. Both equal-width and equal-frequency methods can be found in the <code>sklearn.preprocessing.KBinsDiscretizer</code> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "equal_with_disc = KBinsDiscretizer(n_bins=3, strategy=\"uniform\", encode=\"ordinal\") # 'encode' is the method to encode the transformed result\n",
    "equal_width_data = equal_with_disc.fit_transform(df[[\"horsepower\"]])\n",
    "\n",
    "# Transform the result into a categorical variable and name each bin\n",
    "equal_width_data = pd.DataFrame(equal_width_data, columns=[\"horsepower-binned\"])\n",
    "equal_width_data[\"horsepower-binned\"] = equal_width_data[\"horsepower-binned\"].astype(int)\n",
    "equal_width_data[\"horsepower-binned\"] = pd.Categorical(equal_width_data[\"horsepower-binned\"]).rename_categories([\"low\", \"medium\", \"high\"])\n",
    "\n",
    "# Plot the resulting discrete variable\n",
    "print(\"Cut points: \" + str(equal_with_disc.bin_edges_))\n",
    "plt.hist(equal_width_data[\"horsepower-binned\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "equal_freq_disc = KBinsDiscretizer(n_bins=3, strategy=\"quantile\", encode=\"ordinal\") # 'encode' is the method to encode the transformed result\n",
    "equal_freq_data = equal_freq_disc.fit_transform(df[[\"horsepower\"]])\n",
    "\n",
    "# Transform the result into a categorical variable and name each bin\n",
    "equal_freq_data = pd.DataFrame(equal_freq_data, columns=[\"horsepower-binned\"])\n",
    "equal_freq_data[\"horsepower-binned\"] = equal_freq_data[\"horsepower-binned\"].astype(int)\n",
    "equal_freq_data[\"horsepower-binned\"] = pd.Categorical(equal_freq_data[\"horsepower-binned\"]).rename_categories([\"low\", \"medium\", \"high\"])\n",
    "\n",
    "# Plot the resulting discrete variable\n",
    "print(\"Cut points: \" + str(equal_freq_disc.bin_edges_))\n",
    "plt.hist(equal_freq_data[\"horsepower-binned\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"indicator_variables\">5 -Indicator variables</h2>\n",
    "\n",
    "An indicator variable (or dummy variable) is a numerical variable that represents the presence or absence of some categorical label. \n",
    "\n",
    "#### How many dummy variables?\n",
    "\n",
    "The number of dummy variables required to represent a particular categorical variable depends on the number of values that the categorical variable can assume. To represent a categorical variable that can assume $k$ different values, we would need to define $k - 1$ dummy variables. A $k^{th}$ dummy variable is redundant (it carries no new information) and creates a redundancy problem for the analysis (multicollinearity).\n",
    "\n",
    "#### Why not just use an integer variable?\n",
    "\n",
    "* **Ordinal variables**. Lets assume we have a categorical variable called \"speed\" with \"low\", \"medium\" and \"high\" labels. We may argue that the relation \"low\" < \"medium\" < \"high\" makes sense and therefore, using labels 1, 2 and 3 should not be an issue. However, using labels as 100, 101 and 300000 in place of 1, 2 and 3 would still have the same relationship as \"low\", \"medium\" and \"high\" have. In other words, we do not know how greater is a speed of \"medium\" than a speed of \"low\" and how small it is compared to a \"high\" speed (and if we know, then they should have been recorded with quantitative values). Difference between these labels can potentially affect the model we train.\n",
    "\n",
    "* **Nominal variables**. This case is even worse, because its not possible to assign a quantitative value to variables such as color, brands, types of motor, etc.\n",
    "\n",
    "#### In our data\n",
    "In the Automobile data, we would like to use the information of \"fuel-type\" and \"drive-wheels\", two nominal variables, in a regression model, therefore we need to convert them into indicator variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fuel type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_type_dummy = pd.get_dummies(df[\"fuel-type\"], drop_first = True) # We use drop_first to avoid multicollinearity\n",
    "print(fuel_type_dummy.shape)\n",
    "print(fuel_type_dummy.dtypes) # For a {0,1} variable, we dont need that many bits (int32)\n",
    "fuel_type_dummy.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this is a two-variables example, it could also be done by simply calling the <code>factorize()</code> method, which would return the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fuel-type\"].factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drive-wheels**\n",
    "\n",
    "This one is more interesting because it has 3 values and thus we have to create two dummy variables to fully represent it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_wheels_dummy = pd.get_dummies(df[\"drive-wheels\"], drop_first = True)\n",
    "print(drive_wheels_dummy.shape)\n",
    "print(drive_wheels_dummy.dtypes) \n",
    "drive_wheels_dummy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(drive_wheels_dummy[\"fwd\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now add these variables to our dataset and drop the original categorical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, drive_wheels_dummy], axis=1)\n",
    "df.drop(\"drive-wheels\", axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "#### Books\n",
    "1. Bishop, C. M. (2006). \"Pattern recognition and machine learning\". Springer.\n",
    "2. Murphy, K. P. (2012). \"Machine Learning: A probabilistic perspective\". MIT Press.\n",
    "3. Stockburger D. W. (2016). \"Multivariate statistics: concepts, models and applications\".\n",
    "\n",
    "#### Links\n",
    "1. <a href=\"https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\">Hale, J. (2019). \"Scale, Standardize, or Normalize with Scikit-Learn\"</a>\n",
    "3. <a href=\"http://benalexkeen.com/feature-scaling-with-scikit-learn/\">Keen, B. (2017). \"Feature Scaling with scikit-learn\"</a>\n",
    "4. <a href=\"https://stattrek.com/multiple-regression/dummy-variables.aspx\">Stat Trek. \"Dummy Variables in Regression\"</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
